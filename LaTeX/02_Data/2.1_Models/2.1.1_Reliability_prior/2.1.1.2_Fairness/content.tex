%!TEX root=../../../../root.tex

AI is objective only in the sense of learning what human teaches. The data provided by humans can be highly biased.

Socially unfair behaviours can rise when the given data contains social prejudices; for example, the \textit{Xing} social network gave male persons a better rank than women having better observed 
scores. 

The first thing one should do is be aware of this risk, since it is quite challenging to avoid. Some possible causes are:

\begin{itemize}
	\item \emph{Skewed sample}: a tiny initial bias grows over time, since future observations confirm prediction. Example: police intercept crime more densely in areas they watch.
	\item \emph{Tainted examples}: data produced by a human decision can be biased, and the bias is replicated by the system. Example: the Xing ranking system seen before.
	\item \emph{Sample size disparity}: training data for a minority group is much less than the majority group.
\end{itemize}

In general, assessing data and prior reliability is crucial for any learning-based system.