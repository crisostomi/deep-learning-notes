%!TEX root = ../../../root.tex

Let's try to formalize this idea. Let $D_{\gamma}$ be the decoder, or the generator, parametrized by some parameters $\gamma$, and $\Delta_{\delta}$ be the discriminator. Suppose that we have
\begin{itemize}
	\item $\mathbf{x}$: a \emph{real} sample from the \emph{real} distribution;
	\item $\mathbf{x'}$ = $D_{\gamma}(\mathbf{z})$: a \emph{generated} sample, i.e. the output of the decoder supplied with a latent code $\mathbf{z}$, drawn from some probability distribution defined over the latent space. 
\end{itemize}
Suppose that the distribution of the real data is $p_d(\vb{x})$, while the generated data follows a distribution $p_g(\vb{x})$. 
We say that the generator is performing well if it is
\begin{equation}
    p_g(\mathbf{x'}) \approx p_d(\mathbf{x}).
\end{equation}
On the other hand, the discriminator $\Delta$ is performing well if it is
\begin{equation}
    \begin{cases}
        \Delta_{\delta} (\vb{x}) \approx 1 & \overbrace{\vb{x} \sim p_d(\vb{x})}^{\text{on real data}} \\
        \Delta_{\delta} (\vb{x}') \approx 0 & \underbrace{\vb{x}' \sim p_g(\vb{x}')}_{\text{on fake data}}.
    \end{cases}
    \label{eq:13:1:2:d-obj}
\end{equation}

Let's start from the discriminator. Mathematically, we can define an \emph{objective function} for the discriminator that reflects the general objective defined in \cref{eq:13:1:2:d-obj}:
\begin{equation}
    \label{eq:success_rate}
	\max_\delta \underbrace{\mathbb{E}_{\mathbf{x}} \log \Delta_\delta(\mathbf{x})}_{\text{real data}} + \underbrace{\mathbb{E}_{\mathbf{z}} \log \left( 1 - \Delta_\delta(D_\gamma(\mathbf{z})) \right)}_{\text{fake data}}.
\end{equation}
Let's try to understand what is happening. Recall that the discriminator $\Delta_\delta$ acts as a \emph{binary classifier}, so it outputs a scalar in $[0, 1]$ that is the \emph{probability that a sample is real}. 
\begin{itemize}
    \item $\mathbb{E}_{\mathbf{x}} \Delta_\delta(\mathbf{x})$ is the average prediction that the discriminator makes on real data. We want our discriminator to recognize these samples as real, so we want this value to be as close to $1$ as possible.
    
    \item $\mathbb{E}_{\mathbf{z}} \left( \Delta_\delta(D_\gamma(\mathbf{z})) \right)$ is the average prediction that the classifier makes on data generated by the decoder $D$, when supplied with latent codes $\vb{z}$. We want the classifier to recognize these samples as fake, so we want this value to be as close to $0$ as possible, or, equivalently, we want $\mathbb{E}_{\mathbf{z}} \left( 1 - \Delta_\delta(D_\gamma(\mathbf{z})) \right)$ to be as close to $1$ as possible, so that overall we have a maximization problem.
\end{itemize} 

The $\log$ has its justification, for now we can just say that since it is a monotonically increasing function, maximizing $\mathbb{E}_{\mathbf{x}} \Delta_\delta(\mathbf{x})$ or $\mathbb{E}_{\mathbf{x}} \log \Delta_\delta(\mathbf{x})$ is the same thing. This is called \emph{success rate}, and an optimal classifier would be a global optimizer to \cref{eq:success_rate}.

On the other hand, the generator has an opposite objective: it tries to make \cref{eq:success_rate} as small as possible. This would mean that it is so good that is able to fool the classifier every time. So, it should find the parameters $\gamma$ such that when it produces a sample and gives it to the classifier, then its score is as low as possible.

Mathematically this translates to the following objective function:
\begin{equation}\label{eq:generator}
	\min_\gamma \ \max_\delta \ \mathbb{E}_{\mathbf{x}} \log \Delta_\delta(\mathbf{x}) + \mathbb{E}_{\mathbf{z}} \log \left( 1 - \Delta_\delta(D_\gamma(\mathbf{z})) \right)
\end{equation}
The problem in \cref{eq:generator} is a minimax problem, and in general these are very difficult problems from the point of view of optimization, but we will see that GANs provide a way to optimize this kind of problems.

In the following we will assume that the generated data follows a distribution $\mathbf{x} \sim p_g$, where $p_g$ is parametrized by $\gamma$ (the parameters of the generator), and that the real data follows a distribution $\mathbf{x} \sim p_d$.

Let's consider the discriminator: it has to maximize its score, let's call it $J(\cdot)$, given a generator $G$, over which has no control, so it is a variable for the score:
\begin{align}
    J(G) = \max_\delta \mathbb{E}_{\mathbf{x} \sim p_d} \log \Delta_\delta(\mathbf{x}) + \mathbb{E}_{\mathbf{x} \sim p_g} \log \left( 1 - \Delta_\delta(\mathbf{x}) \right).
    \label{eq:13:1:2:d-functional}
\end{align}
Now, let's write explicitly the expectations
\begin{equation}
    J(G) = \max_\delta \int \left[ \log \Delta_\delta(\mathbf{x})p_d(\mathbf{x}) + \log \left( 1 - \Delta_\delta(\mathbf{x})\right)p_g(\mathbf{x}) \right] d\mathbf{x} 
\end{equation}
Now we reason pointwise, i.e. we operate outside the integral on an element $\mathbf{x}$ at a time. For any given $\mathbf{x}$, we want to maximize $\Delta_\delta(\mathbf{x}) = a$. Let's rename for simplicity $p_d(\mathbf{x}) \equiv p$ and $p_g(\mathbf{x}) \equiv q$. Then for a single $\mathbf{x}$ the score is:
\begin{equation}\label{eq:short-max-discr}
	\max_a p \log(a) + q \log(1-a).
\end{equation}
We can see that \cref{eq:short-max-discr} is a concave function, meaning that we can find the maximum just by taking the gradient and equating the gradient to zero:
\begin{align}
	\frac{p}{a} - \frac{q}{1-a} &= 0 \\
	a &= \frac{p}{p + q}
\end{align}

We thus have a closed-form solution for the optimal discriminator on a single sample:
\begin{equation}
	\Delta_\delta(\mathbf{x}) = \frac{p_d(\mathbf{x})}{p_d(\mathbf{x}) + p_g(\mathbf{x})}.
\end{equation}

Since we did not assume anything about the sample, this must hold for any general sample, so we can plug this expression back in \cref{eq:13:1:2:d-functional}, to get:
\begin{align}
    J(G) = \max_\delta \mathbb{E}_{\mathbf{x} \sim p_d} \log \frac{p_d(\mathbf{x})}{p_d(\mathbf{x}) + p_g(\mathbf{x})} + \mathbb{E}_{\mathbf{x} \sim p_g} \log \frac{p_g(\mathbf{x})}{p_d(\mathbf{x}) + p_g(\mathbf{x})}.
    \label{eq:13:1:2:d-eq}
\end{align}

Let's now define a new distribution, that behaves like a ``midpoint distribution'' between $p_d$ and $p_g$, i.e. it is defined point-wise as
\begin{equation}
	\rho = \frac12p_d + \frac12p_g.
\end{equation}

Then, we can rewrite \cref{eq:13:1:2:d-eq} as
\begin{equation}
    J(G) = \max_\delta \frac12 \mathbb{E}_{\mathbf{x} \sim p_d} \log \frac{p_d(\mathbf{x})}{\rho(\mathbf{x})} + \frac12 \mathbb{E}_{\mathbf{x} \sim p_g} \log \frac{p_g(\mathbf{x})}{\rho(\mathbf{x})} + \mathrm{const}
\end{equation}
in which we recognize $\mathbb{E}_{\mathbf{x} \sim p_d} \log \frac{p_d(\mathbf{x})}{\rho(\mathbf{x})}$ and $\mathbb{E}_{\mathbf{x} \sim p_g} \log \frac{p_g(\mathbf{x})}{\rho(\mathbf{x})}$ as Kullback-Leibler divergences, so we write:
\begin{equation}
	J(G) = \max_\delta \frac12 KL(p_d \| \rho) + \frac12 KL(p_g \| \rho) + \mathrm{const}.
\end{equation}

Now, this is the expression of the score of the optimal discriminator, given a generator $G$. Recall, the generator wants to to minimize the discriminator score, therefore the optimal GAN generator is found by minimizing:
\begin{equation}
	\min_{p_g} \frac12 KL(p_d \| \rho) + \frac12 KL(p_g \| \rho) + \mathrm{const}
\end{equation}
and since the constant is just a shift in the energy profile of this optimization problem and it does not depend on $p_g$ we can discard it:
\begin{equation}
    \min_{p_g} \frac12 KL(p_d \| \rho) + \frac12 KL(p_g \| \rho).
    \label{eq:13:1:2:g-objective}
\end{equation}

The most notable thing about this expression is that it is the definition of another divergence measure over probability distributions: the \emph{Jensen-Shannon divergence} between $p_d$ and $p_g$. 

Like the $KL$ divergence, it is not actually a distance because it is not symmetric but the interesting thing is that it is ``almost a distance''. In fact, it satisfies a \emph{defining property} for distance functions, namely the property that if the two distributions $p_d, p_g$ are equal, so they are actually the same distribution, then the Jensen-Shannon divergence is equal to zero and vice versa; in metric spaces this is called identity of indiscernibles. 

This, in the GAN setting, means that the globally optimal generator has a distribution exactly equal to the real distribution of the data, and this is exactly what we wanted. By minimizing \cref{eq:13:1:2:g-objective} we are pushing $p_g$ to be more and more similar to $p_d$, so that in the irrealistic hypothesis of finding an optimal generator, they would be exactly equal. In practice, it is \emph{very hard} to train correctly a GAN, since there are numerous problems that can arise. For instance, competition between the two models can very easily tip in favor of one of the two, that effortlessly beats the other in a way that is not informative for the other to improve, and so the training is stuck.
