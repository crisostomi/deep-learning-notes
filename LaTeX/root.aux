\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\pbs@newkey[2]{}
\providecommand\pbs@seq@push@cx[2]{}
\providecommand\pbs@at@end@dvi@check{}
\pbs@at@end@dvi@check
\providecommand\@anim@newkey[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\pbs@newkey{pbs@last@page}{1}
\providecommand \oddpage@label [2]{}
\pbs@newkey{pbs@last@page}{2}
\pbs@newkey{pbs@last@page}{3}
\pbs@newkey{pbs@last@page}{4}
\pbs@newkey{pbs@last@page}{5}
\pbs@newkey{pbs@last@page}{6}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Data}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Structured quartet.\relax }}{5}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:anscombe-quartet}{{1.1}{5}{Structured quartet.\relax }{figure.caption.3}{}}
\newlabel{fig:anscombe-quartet@cref}{{[figure][1][1]1.1}{[1][5][]5}}
\pbs@newkey{pbs@last@page}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Unstructured quartet.\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:anscombe-unstr-quartet}{{1.2}{6}{Unstructured quartet.\relax }{figure.caption.4}{}}
\newlabel{fig:anscombe-unstr-quartet@cref}{{[figure][2][1]1.2}{[1][5][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Datasets with the same statistics.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:dozen-datasets}{{1.3}{6}{Datasets with the same statistics.\relax }{figure.caption.5}{}}
\newlabel{fig:dozen-datasets@cref}{{[figure][3][1]1.3}{[1][6][]6}}
\pbs@newkey{pbs@last@page}{8}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Models for describing the data}{7}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Reliability of the prior}{7}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Black hole imaging}{7}{section*.7}\protected@file@percent }
\pbs@newkey{pbs@last@page}{9}
\@writefile{toc}{\contentsline {subsubsection}{Fairness}{8}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Explaining the data}{8}{subsection.1.1.2}\protected@file@percent }
\pbs@newkey{pbs@last@page}{10}
\@writefile{toc}{\contentsline {paragraph}{Choosing a representation}{9}{section*.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces A linear curve.\relax }}{9}{figure.caption.11}\protected@file@percent }
\newlabel{fig:linear}{{1.4}{9}{A linear curve.\relax }{figure.caption.11}{}}
\newlabel{fig:linear@cref}{{[figure][4][1]1.4}{[1][8][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces A spiral-shape dataset.\relax }}{9}{figure.caption.12}\protected@file@percent }
\newlabel{fig:spiral}{{1.5}{9}{A spiral-shape dataset.\relax }{figure.caption.12}{}}
\newlabel{fig:spiral@cref}{{[figure][5][1]1.5}{[1][9][]9}}
\pbs@newkey{pbs@last@page}{11}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}The curse of dimensionality}{10}{section.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces $1 \mkern 1.5mu{\times }\mkern 1.5mu1$ images on the real axis.\relax }}{10}{figure.caption.13}\protected@file@percent }
\newlabel{fig:1x1imgs}{{1.6}{10}{$1 \shorttimes 1$ images on the real axis.\relax }{figure.caption.13}{}}
\newlabel{fig:1x1imgs@cref}{{[figure][6][1]1.6}{[1][10][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces $2 \times 1$ images on the real axis.\relax }}{10}{figure.caption.14}\protected@file@percent }
\newlabel{fig:2x1imgs}{{1.7}{10}{$2 \times 1$ images on the real axis.\relax }{figure.caption.14}{}}
\newlabel{fig:2x1imgs@cref}{{[figure][7][1]1.7}{[1][10][]10}}
\pbs@newkey{pbs@last@page}{12}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces $3 \mkern 1.5mu{\times }\mkern 1.5mu1$ images on the real axis.\relax }}{11}{figure.caption.15}\protected@file@percent }
\newlabel{fig:3x1imgs}{{1.8}{11}{$3 \shorttimes 1$ images on the real axis.\relax }{figure.caption.15}{}}
\newlabel{fig:3x1imgs@cref}{{[figure][8][1]1.8}{[1][10][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Decrease the dimensions}{11}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Features}{11}{section*.16}\protected@file@percent }
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {ex}{\numberline {1.1}Example}{11}{ex.1.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{13}
\@writefile{loe}{\contentsline {ex}{\numberline {1.2}Example}{12}{ex.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intrinsic invariances}{12}{section*.17}\protected@file@percent }
\@writefile{loe}{\contentsline {ex}{\numberline {1.3}Example}{12}{ex.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Three different embeddings of the same object.\relax }}{12}{figure.caption.18}\protected@file@percent }
\pbs@newkey{pbs@last@page}{14}
\@writefile{toc}{\contentsline {paragraph}{Latent features}{13}{section*.19}\protected@file@percent }
\@writefile{loe}{\contentsline {ex}{\numberline {1.4}Example}{13}{ex.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Example of a latent feature: directional illumination.\relax }}{13}{figure.caption.20}\protected@file@percent }
\newlabel{fig:lights}{{1.10}{13}{Example of a latent feature: directional illumination.\relax }{figure.caption.20}{}}
\newlabel{fig:lights@cref}{{[figure][10][1]1.10}{[1][13][]13}}
\@writefile{toc}{\contentsline {paragraph}{Dimensionality}{13}{section*.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces The Hughes phenomenon.\relax }}{13}{figure.caption.22}\protected@file@percent }
\newlabel{fig:hughes}{{1.11}{13}{The Hughes phenomenon.\relax }{figure.caption.22}{}}
\newlabel{fig:hughes@cref}{{[figure][11][1]1.11}{[1][13][]13}}
\pbs@newkey{pbs@last@page}{15}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Swiss roll plot.\relax }}{14}{figure.caption.23}\protected@file@percent }
\newlabel{fig:roll}{{1.12}{14}{Swiss roll plot.\relax }{figure.caption.23}{}}
\newlabel{fig:roll@cref}{{[figure][12][1]1.12}{[1][14][]14}}
\@writefile{loe}{\contentsline {defn}{\numberline {1.1}Definition\thmtformatoptarg {Manifold}}{14}{defn.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces A manifold.\relax }}{14}{figure.caption.24}\protected@file@percent }
\newlabel{fig:manifold}{{1.13}{14}{A manifold.\relax }{figure.caption.24}{}}
\newlabel{fig:manifold@cref}{{[figure][13][1]1.13}{[1][14][]14}}
\@writefile{toc}{\contentsline {paragraph}{Task-driven features}{14}{section*.25}\protected@file@percent }
\@writefile{loe}{\contentsline {defn}{\numberline {1.2}Definition\thmtformatoptarg {Deep learning}}{14}{defn.1.2}\protected@file@percent }
\pbs@newkey{pbs@last@page}{16}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Linear regression, convexity and gradients}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of a neural network structure\relax }}{15}{figure.caption.26}\protected@file@percent }
\newlabel{fig:nn-structure}{{2.1}{15}{Example of a neural network structure\relax }{figure.caption.26}{}}
\newlabel{fig:nn-structure@cref}{{[figure][1][2]2.1}{[1][15][]15}}
\pbs@newkey{pbs@last@page}{17}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Linear Regression}{16}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Linear regression problem.\relax }}{16}{figure.caption.27}\protected@file@percent }
\newlabel{fig:lin-reg}{{2.2}{16}{Linear regression problem.\relax }{figure.caption.27}{}}
\newlabel{fig:lin-reg@cref}{{[figure][2][2]2.2}{[1][16][]16}}
\pbs@newkey{pbs@last@page}{18}
\pbs@newkey{pbs@last@page}{19}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Task of a deep neural network.\relax }}{18}{figure.caption.28}\protected@file@percent }
\newlabel{fig:nn-task}{{2.3}{18}{Task of a deep neural network.\relax }{figure.caption.28}{}}
\newlabel{fig:nn-task@cref}{{[figure][3][2]2.3}{[1][17][]18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Convexity}{18}{section.2.2}\protected@file@percent }
\newlabel{eq:convexity}{{2.10}{18}{Convexity}{equation.2.2.10}{}}
\newlabel{eq:convexity@cref}{{[equation][10][2]2.10}{[1][18][]18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Jensen's inequality generalizes the statement that a secant line of a convex function lies above the graph.\relax }}{18}{figure.caption.29}\protected@file@percent }
\newlabel{fig:jensen}{{2.4}{18}{Jensen's inequality generalizes the statement that a secant line of a convex function lies above the graph.\relax }{figure.caption.29}{}}
\newlabel{fig:jensen@cref}{{[figure][4][2]2.4}{[1][18][]18}}
\pbs@newkey{pbs@last@page}{20}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Taylor approximation.\relax }}{19}{figure.caption.32}\protected@file@percent }
\newlabel{fig:taylor}{{2.5}{19}{Taylor approximation.\relax }{figure.caption.32}{}}
\newlabel{fig:taylor@cref}{{[figure][5][2]2.5}{[1][19][]19}}
\pbs@newkey{pbs@last@page}{21}
\newlabel{eq:minimizer}{{2.18}{20}{Convexity}{equation.2.2.18}{}}
\newlabel{eq:minimizer@cref}{{[equation][18][2]2.18}{[1][20][]20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Gradients}{20}{section.2.3}\protected@file@percent }
\pbs@newkey{pbs@last@page}{22}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The gradient points towards the direction of steepest ascent.\relax }}{21}{figure.caption.33}\protected@file@percent }
\newlabel{fig:gradient}{{2.6}{21}{The gradient points towards the direction of steepest ascent.\relax }{figure.caption.33}{}}
\newlabel{fig:gradient@cref}{{[figure][6][2]2.6}{[1][21][]21}}
\newlabel{eq:4:3:dir_der_grad}{{2.26}{21}{Gradients}{equation.2.3.26}{}}
\newlabel{eq:4:3:dir_der_grad@cref}{{[equation][26][2]2.26}{[1][21][]21}}
\pbs@newkey{pbs@last@page}{23}
\pbs@newkey{pbs@last@page}{24}
\newlabel{eq:lr-norm-eq}{{2.39}{23}{Gradients}{equation.2.3.39}{}}
\newlabel{eq:lr-norm-eq@cref}{{[equation][39][2]2.39}{[1][23][]23}}
\newlabel{eq:lin_reg}{{2.44}{23}{Gradients}{equation.2.3.44}{}}
\newlabel{eq:lin_reg@cref}{{[equation][44][2]2.44}{[1][23][]23}}
\pbs@newkey{pbs@last@page}{25}
\pbs@newkey{pbs@last@page}{26}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Nonlinear models, overfitting and regularization}{25}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces More data confutes the linear assumption.\relax }}{25}{figure.caption.38}\protected@file@percent }
\newlabel{fig:conf_assump}{{3.1}{25}{More data confutes the linear assumption.\relax }{figure.caption.38}{}}
\newlabel{fig:conf_assump@cref}{{[figure][1][3]3.1}{[1][25][]25}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Nonlinear models}{25}{section.3.1}\protected@file@percent }
\newlabel{eq:poly}{{3.1}{25}{Nonlinear models}{equation.3.1.1}{}}
\newlabel{eq:poly@cref}{{[equation][1][3]3.1}{[1][25][]25}}
\pbs@newkey{pbs@last@page}{27}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Polynomial regression.\relax }}{26}{figure.caption.39}\protected@file@percent }
\newlabel{fig:poly_regr}{{3.2}{26}{Polynomial regression.\relax }{figure.caption.39}{}}
\newlabel{fig:poly_regr@cref}{{[figure][2][3]3.2}{[1][25][]26}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Polynomial fitting}{26}{section.3.2}\protected@file@percent }
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {unboxedthm}{\numberline {3.1}Theorem\thmtformatoptarg {Stone-Weierstrass}}{26}{unboxedthm.3.1}\protected@file@percent }
\newlabel{thm:stone_weiestrass}{{3.1}{26}{Stone-Weierstrass}{unboxedthm.3.1}{}}
\newlabel{thm:stone_weiestrass@cref}{{[unboxedthm][1][3]3.1}{[1][26][]26}}
\pbs@newkey{pbs@last@page}{28}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Fit data with different poly degree.\relax }}{27}{figure.caption.40}\protected@file@percent }
\newlabel{fig:fit_poly}{{3.3}{27}{Fit data with different poly degree.\relax }{figure.caption.40}{}}
\newlabel{fig:fit_poly@cref}{{[figure][3][3]3.3}{[1][26][]27}}
\pbs@newkey{pbs@last@page}{29}
\@writefile{toc}{\contentsline {paragraph}{$k$-fold cross-validation}{28}{section*.41}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces k-fold cross validation.\relax }}{28}{figure.caption.42}\protected@file@percent }
\newlabel{fig_cross_valid}{{3.4}{28}{k-fold cross validation.\relax }{figure.caption.42}{}}
\newlabel{fig_cross_valid@cref}{{[figure][4][3]3.4}{[1][28][]28}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Regularization}{28}{section.3.3}\protected@file@percent }
\pbs@newkey{pbs@last@page}{30}
\@writefile{loe}{\contentsline {defn}{\numberline {3.1}Definition\thmtformatoptarg {Entry-wise norms}}{29}{defn.3.1}\protected@file@percent }
\newlabel{def:entry-norm}{{3.1}{29}{Entry-wise norms}{defn.3.1}{}}
\newlabel{def:entry-norm@cref}{{[defn][1][3]3.1}{[1][29][]29}}
\pbs@newkey{pbs@last@page}{31}
\@writefile{loe}{\contentsline {defn}{\numberline {3.2}Definition\thmtformatoptarg {Regularization}}{30}{defn.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Classification}{30}{section.3.4}\protected@file@percent }
\pbs@newkey{pbs@last@page}{32}
\@writefile{toc}{\contentsline {paragraph}{Logistic regression}{31}{section*.43}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Logistic sigmoid.\relax }}{31}{figure.caption.44}\protected@file@percent }
\newlabel{fig:sigmoid}{{3.5}{31}{Logistic sigmoid.\relax }{figure.caption.44}{}}
\newlabel{fig:sigmoid@cref}{{[figure][5][3]3.5}{[1][31][]31}}
\pbs@newkey{pbs@last@page}{33}
\newlabel{eq:der-sig}{{3.29}{32}{Logistic regression}{equation.3.4.29}{}}
\newlabel{eq:der-sig@cref}{{[equation][29][3]3.29}{[1][32][]32}}
\pbs@newkey{pbs@last@page}{34}
\pbs@newkey{pbs@last@page}{35}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Stochastic gradient descent}{34}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{34}{section.4.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{36}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Plot of a function from $\mathbb  {R}^2$ to $\mathbb  {R}$.\relax }}{35}{figure.caption.45}\protected@file@percent }
\newlabel{fig:06:1:gradient}{{4.1}{35}{Plot of a function from $\mathbb {R}^2$ to $\mathbb {R}$.\relax }{figure.caption.45}{}}
\newlabel{fig:06:1:gradient@cref}{{[figure][1][4]4.1}{[1][35][]35}}
\newlabel{eq:chap6:intro:update-law}{{4.1}{35}{Introduction}{equation.4.1.1}{}}
\newlabel{eq:chap6:intro:update-law@cref}{{[equation][1][4]4.1}{[1][35][]35}}
\pbs@newkey{pbs@last@page}{37}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Gradient Properties}{36}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Orthogonality and steepest ascent}{36}{subsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Computing the gradient of the function at ``every'' point of the domain leads to a \emph  {vector field}, that visualizes the ``flow'' of the function, from points of lower values to points of higher values. We seek to follow the opposite flow. Indeed, in the figures the negative gradient is plotted, with regions with higher density having points with larger gradients in magnitude.\relax }}{36}{figure.caption.46}\protected@file@percent }
\pbs@newkey{pbs@last@page}{38}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Relation between isocurves, directional derivatives, gradient, and function increase.\relax }}{37}{figure.caption.47}\protected@file@percent }
\newlabel{fig:chap6:dirder-gradient}{{4.3}{37}{Relation between isocurves, directional derivatives, gradient, and function increase.\relax }{figure.caption.47}{}}
\newlabel{fig:chap6:dirder-gradient@cref}{{[figure][3][4]4.3}{[1][36][]37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Differentiability}{37}{subsection.4.2.2}\protected@file@percent }
\pbs@newkey{pbs@last@page}{39}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Stationary points}{38}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Learning Rate}{38}{section.4.3}\protected@file@percent }
\pbs@newkey{pbs@last@page}{40}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparison of values of the learning rate $\alpha $.\relax }}{39}{figure.caption.49}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Decay}{39}{subsection.4.3.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{41}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Momentum}{40}{subsection.4.3.2}\protected@file@percent }
\pbs@newkey{pbs@last@page}{42}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Acceleration effect of momentum in situations with small step size.\relax }}{41}{figure.caption.50}\protected@file@percent }
\newlabel{fig:chap6:momentum}{{4.5}{41}{Acceleration effect of momentum in situations with small step size.\relax }{figure.caption.50}{}}
\newlabel{fig:chap6:momentum@cref}{{[figure][5][4]4.5}{[1][41][]41}}
\pbs@newkey{pbs@last@page}{43}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Gradient Descent for Deep Learning: Stochastic Gradient Descent}{42}{section.4.4}\protected@file@percent }
\pbs@newkey{pbs@last@page}{44}
\pbs@newkey{pbs@last@page}{45}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Empirical observation of oscillatory behavior at convergence.\relax }}{44}{figure.caption.51}\protected@file@percent }
\pbs@newkey{pbs@last@page}{46}
\pbs@newkey{pbs@last@page}{47}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Multi-layer perceptron and back-propagation}{46}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{46}{section.5.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{48}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Deep networks}{47}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Deep composition}{47}{subsection.5.2.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{49}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}MLP}{48}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Hidden units}{48}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Single layer illustration}{48}{section*.53}\protected@file@percent }
\pbs@newkey{pbs@last@page}{50}
\@writefile{toc}{\contentsline {subsubsection}{Output layer}{49}{section*.54}\protected@file@percent }
\pbs@newkey{pbs@last@page}{51}
\@writefile{toc}{\contentsline {subsubsection}{Deep ReLU networks}{50}{section*.55}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Activation functions.\relax }}{50}{figure.caption.56}\protected@file@percent }
\newlabel{fig:07:2:2:4:act_funcs}{{5.1}{50}{Activation functions.\relax }{figure.caption.56}{}}
\newlabel{fig:07:2:2:4:act_funcs@cref}{{[figure][1][5]5.1}{[1][50][]50}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Landscape of the function for a 2-layer deep ReLU network. Best viewed in color.\relax }}{50}{figure.caption.57}\protected@file@percent }
\pbs@newkey{pbs@last@page}{52}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Universality}{51}{subsection.5.2.3}\protected@file@percent }
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {unboxedthm}{\numberline {5.1}Theorem\thmtformatoptarg {Universal Approximation Theorem}}{51}{unboxedthm.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Training}{51}{section.5.3}\protected@file@percent }
\pbs@newkey{pbs@last@page}{53}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Computational graphs}{52}{subsection.5.3.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{54}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Automatic differentiation: forward mode}{53}{subsection.5.3.2}\protected@file@percent }
\newlabel{eq:07:3:2:forward}{{5.29}{53}{Automatic differentiation: forward mode}{equation.5.3.29}{}}
\newlabel{eq:07:3:2:forward@cref}{{[equation][29][5]5.29}{[1][53][]53}}
\pbs@newkey{pbs@last@page}{55}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Automatic differentiation: reverse mode}{54}{subsection.5.3.3}\protected@file@percent }
\newlabel{eq:07:3:2:reverse}{{5.30}{54}{Automatic differentiation: reverse mode}{equation.5.3.30}{}}
\newlabel{eq:07:3:2:reverse@cref}{{[equation][30][5]5.30}{[1][54][]54}}
\pbs@newkey{pbs@last@page}{56}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Automatic differentiation: complexity}{55}{subsection.5.3.4}\protected@file@percent }
\newlabel{eq:07:3:4:jacobian}{{5.35}{55}{Automatic differentiation: complexity}{equation.5.3.35}{}}
\newlabel{eq:07:3:4:jacobian@cref}{{[equation][35][5]5.35}{[1][55][]55}}
\pbs@newkey{pbs@last@page}{57}
\pbs@newkey{pbs@last@page}{58}
\pbs@newkey{pbs@last@page}{59}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.5}Backpropagation}{58}{subsection.5.3.5}\protected@file@percent }
\newlabel{eq:07:3:5:grad}{{5.48}{58}{Backpropagation}{equation.5.3.48}{}}
\newlabel{eq:07:3:5:grad@cref}{{[equation][48][5]5.48}{[1][58][]58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.6}Observations}{58}{subsection.5.3.6}\protected@file@percent }
\pbs@newkey{pbs@last@page}{60}
\pbs@newkey{pbs@last@page}{61}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Convolutional neural networks}{60}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Deep feed-forward neural network consisting of $L$ layers.\relax }}{60}{figure.caption.58}\protected@file@percent }
\pbs@newkey{pbs@last@page}{62}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Need for Priors}{61}{section.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Examples of \textit  {structure}; we want to take advantage of this structure by means of \emph  {priors}.\relax }}{61}{figure.caption.59}\protected@file@percent }
\pbs@newkey{pbs@last@page}{63}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Self-similarity}{62}{subsection.6.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Self-similarity means less information needed to be represented by the network. Self-similarity also means data is \textit  {predictable}: we expect the fence and the vegetation to continue behind the eagle, lazily repeating itself, and so does the network, that is able to remove it from the image and fill in the gap in a credible way.\relax }}{62}{figure.caption.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Translation invariance}{62}{subsection.6.1.2}\protected@file@percent }
\pbs@newkey{pbs@last@page}{64}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces A cat is still a cat regardless of where it is located in an image.\relax }}{63}{figure.caption.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Other invariances}{63}{subsection.6.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces A digit is still a digit regardless of how much (within some degree) deformed it appears in images of handwritten text.\relax }}{63}{figure.caption.62}\protected@file@percent }
\pbs@newkey{pbs@last@page}{65}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Even if these two models are displaced (deformed) differently, and miss some parts (therefore are partials), we still recognize both of them as dogs.\relax }}{64}{figure.caption.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Hierarchy and compositionality}{64}{subsection.6.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces A human face is still a human face even when translated, and so is a nose or an eye, and so are the edges that make up each of those, provided that when arranged together, the local features at a lower scale \textit  {compose} a correct local feature at a higher scale.\relax }}{64}{figure.caption.64}\protected@file@percent }
\pbs@newkey{pbs@last@page}{66}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Convolution}{65}{section.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Operative illustration of convolution.\relax }}{65}{figure.caption.65}\protected@file@percent }
\pbs@newkey{pbs@last@page}{67}
\zref@newlabel{anim@abspage0}{\abspage{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Convolution of two identical ``pulse'' functions. If the animation is not displayed correctly, try to enable JavaScript in your PDF reader or use Adobe Reader which has the correct extensions already enabled.\relax }}{66}{figure.caption.66}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Properties}{66}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Commutativity}{66}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Shift-equivariance}{66}{section*.68}\protected@file@percent }
\pbs@newkey{pbs@last@page}{68}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Convolution can be applied to images, as we will see shortly, and exhibits shift-equivariance.\relax }}{67}{figure.caption.69}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Linearity}{67}{section*.70}\protected@file@percent }
\pbs@newkey{pbs@last@page}{69}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Discrete Convolution}{68}{subsection.6.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Visualization of discrete convolution on two sequences $f, g$.\relax }}{68}{figure.caption.71}\protected@file@percent }
\newlabel{eq:08:03:conv-sum}{{6.20}{68}{Discrete Convolution}{equation.6.2.20}{}}
\newlabel{eq:08:03:conv-sum@cref}{{[equation][20][6]6.20}{[1][68][]68}}
\pbs@newkey{pbs@last@page}{70}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Visual interpretation of $2D$ convolution of an image with a kernel as the action of \emph  {sliding} a \emph  {moving window} onto the image.\relax }}{69}{figure.caption.72}\protected@file@percent }
\zref@newlabel{anim@abspage1}{\abspage{70}}
\zref@newlabel{anim@abspage2}{\abspage{70}}
\zref@newlabel{anim@abspage3}{\abspage{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Boundary conditions and stride.\relax }}{69}{figure.caption.73}\protected@file@percent }
\pbs@newkey{pbs@last@page}{71}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Convolutional Neural Networks}{70}{section.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces CNN consisting of $L$ convolutional layers. Notice the architecture superficially is identical to a feed-forward neural network.\relax }}{70}{figure.caption.74}\protected@file@percent }
\pbs@newkey{pbs@last@page}{72}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Visualization of weight sharing.\relax }}{71}{figure.caption.75}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Pooling operation.\relax }}{71}{figure.caption.76}\protected@file@percent }
\pbs@newkey{pbs@last@page}{73}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces CNN consisting of $L$ convolutional layers interleaved with pooling\relax }}{72}{figure.caption.77}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Convolution and downsampling (pooling) operations give the typical shape of a CNN, with data decreasing in size but filters increasing in number, then followed by fully-connected layers to perform other tasks like classification, based on the representation of the data the previous layers have extracted.\relax }}{72}{figure.caption.78}\protected@file@percent }
\pbs@newkey{pbs@last@page}{74}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Empirically it has been shown that it is possible to interpret the CNN architecture as learning increasingly more complex features.\relax }}{73}{figure.caption.79}\protected@file@percent }
\pbs@newkey{pbs@last@page}{75}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Regularization}{74}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Overfitting}{74}{section*.80}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{General idea}{74}{section*.81}\protected@file@percent }
\pbs@newkey{pbs@last@page}{76}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Explicit ways of regularization}{75}{section.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces An histogram of the values taken by the weights shows how this regularizer tends to minimize an $x^2$ penalty for each weight, promoting \emph  {shrinkage}. Notice how the quadratic penalty translates to having higher penalty on weights with larger norms, and viceversa.\relax }}{75}{figure.caption.82}\protected@file@percent }
\pbs@newkey{pbs@last@page}{77}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces An histogram of the values taken by the weights shows how this regularizer tends to minimize an $x$ penalty for each weight, promoting \emph  {sparsity} or \emph  {weight selection}. Notice how every weight receives (proportionally) an equal penalty, and so most of them are at zero.\relax }}{76}{figure.caption.83}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sparsity}{76}{section*.84}\protected@file@percent }
\newlabel{eq:9:1:constraint}{{7.4}{76}{Sparsity}{equation.7.1.4}{}}
\newlabel{eq:9:1:constraint@cref}{{[equation][4][7]7.4}{[1][76][]76}}
\pbs@newkey{pbs@last@page}{78}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces $L_1$ penalty vs $L_2$ penalty.\relax }}{77}{figure.caption.85}\protected@file@percent }
\newlabel{fig:09:1:penalties}{{7.3}{77}{$L_1$ penalty vs $L_2$ penalty.\relax }{figure.caption.85}{}}
\newlabel{fig:09:1:penalties@cref}{{[figure][3][7]7.3}{[1][76][]77}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Early stopping}{77}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{U-shaped curve}{77}{section*.86}\protected@file@percent }
\pbs@newkey{pbs@last@page}{79}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Plot of the validation error when increasing the training time showing a typical U-shape. Each curve corresponds to a different model size (increasing from top to bottom).\relax }}{78}{figure.caption.87}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Overfitting}{78}{section*.88}\protected@file@percent }
\pbs@newkey{pbs@last@page}{80}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces More MLP parameters \emph  {not} leading to overfitting.\relax }}{79}{figure.caption.89}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Good fit over all the different \emph  {data regions}.\relax }}{79}{figure.caption.90}\protected@file@percent }
\pbs@newkey{pbs@last@page}{81}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Example of overfitting and underfitting in localized \emph  {data regions}.\relax }}{80}{figure.caption.91}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Two important requirements for \emph  {early stopping}.\relax }}{80}{figure.caption.92}\protected@file@percent }
\pbs@newkey{pbs@last@page}{82}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Plot of the training and validation loss as a function of model \emph  {capacity} $\mathcal  {H}$.\relax }}{81}{figure.caption.94}\protected@file@percent }
\newlabel{fig:09:2:capacity-u-shape}{{7.9}{81}{Plot of the training and validation loss as a function of model \emph {capacity} $\mathcal {H}$.\relax }{figure.caption.94}{}}
\newlabel{fig:09:2:capacity-u-shape@cref}{{[figure][9][7]7.9}{[1][80][]81}}
\@writefile{toc}{\contentsline {paragraph}{Double descent (Capacity-wise double U-shape)}{81}{figure.caption.94}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces Plot of \emph  {double descent} curve.\relax }}{81}{figure.caption.95}\protected@file@percent }
\newlabel{fig:09:2:double-u-shape}{{7.10}{81}{Plot of \emph {double descent} curve.\relax }{figure.caption.95}{}}
\newlabel{fig:09:2:double-u-shape@cref}{{[figure][10][7]7.10}{[1][81][]81}}
\pbs@newkey{pbs@last@page}{83}
\@writefile{toc}{\contentsline {paragraph}{Epoch-wise double U-shape}{82}{section*.96}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces Train error and test error, plotted both against number of parameters and against training time.\relax }}{82}{figure.caption.97}\protected@file@percent }
\newlabel{fig:09:2:double-u-time}{{7.11}{82}{Train error and test error, plotted both against number of parameters and against training time.\relax }{figure.caption.97}{}}
\newlabel{fig:09:2:double-u-time@cref}{{[figure][11][7]7.11}{[1][82][]82}}
\pbs@newkey{pbs@last@page}{84}
\@writefile{toc}{\contentsline {paragraph}{How early stopping acts as a regularizer}{83}{section*.98}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Batch normalization}{83}{section.7.3}\protected@file@percent }
\pbs@newkey{pbs@last@page}{85}
\@writefile{toc}{\contentsline {paragraph}{The transformation}{84}{section*.99}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learnable parameters: Scale and shift}{84}{section*.100}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Using mini-batches}{84}{section*.101}\protected@file@percent }
\pbs@newkey{pbs@last@page}{86}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Statistics calculation on mini-batches.\relax }}{85}{figure.caption.102}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Properties}{85}{section*.103}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variants}{85}{section*.104}\protected@file@percent }
\pbs@newkey{pbs@last@page}{87}
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Visualization of the action of batchnorm and its variants on the incoming batches of matrices.\relax }}{86}{figure.caption.105}\protected@file@percent }
\newlabel{fig:09:3:batchnorm-visualization}{{7.13}{86}{Visualization of the action of batchnorm and its variants on the incoming batches of matrices.\relax }{figure.caption.105}{}}
\newlabel{fig:09:3:batchnorm-visualization@cref}{{[figure][13][7]7.13}{[1][85][]86}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Dropout}{86}{section.7.4}\protected@file@percent }
\pbs@newkey{pbs@last@page}{88}
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces Before and after dropout.\relax }}{87}{figure.caption.106}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implicit ensembles}{87}{section*.107}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Training}{87}{section*.108}\protected@file@percent }
\pbs@newkey{pbs@last@page}{89}
\@writefile{lof}{\contentsline {figure}{\numberline {7.15}{\ignorespaces New models are generated one by one as one produces new mini-batches during optimization. Notice the weight sharing, e.g. the blue edges have the same weights in every model that contains them.\relax }}{88}{figure.caption.109}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Testing}{88}{section*.110}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.16}{\ignorespaces Dropout: training time vs test time.\relax }}{88}{figure.caption.111}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Properties as a regularizer}{88}{section*.112}\protected@file@percent }
\pbs@newkey{pbs@last@page}{90}
\@writefile{lof}{\contentsline {figure}{\numberline {7.17}{\ignorespaces Validation error comparison.\relax }}{89}{figure.caption.113}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Dropout as an explicit penalty}{89}{section*.114}\protected@file@percent }
\pbs@newkey{pbs@last@page}{91}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Deep generative models}{90}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Dimensionality reduction}{90}{section*.115}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Principal component analysis}{90}{section.8.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{92}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces An example of PCA.\relax }}{91}{figure.caption.116}\protected@file@percent }
\newlabel{fig:pca-2d}{{8.1}{91}{An example of PCA.\relax }{figure.caption.116}{}}
\newlabel{fig:pca-2d@cref}{{[figure][1][8]8.1}{[1][91][]91}}
\pbs@newkey{pbs@last@page}{93}
\newlabel{eq:projection}{{8.11}{92}{Principal component analysis}{equation.8.1.11}{}}
\newlabel{eq:projection@cref}{{[equation][11][8]8.11}{[1][92][]92}}
\newlabel{eq:reconstruction}{{8.12}{92}{Principal component analysis}{equation.8.1.12}{}}
\newlabel{eq:reconstruction@cref}{{[equation][12][8]8.12}{[1][92][]92}}
\pbs@newkey{pbs@last@page}{94}
\zref@newlabel{anim@abspage4}{\abspage{94}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces The direction along which the projection error is minimized is also the one over which variance is maximized.\relax }}{93}{figure.caption.117}\protected@file@percent }
\pbs@newkey{pbs@last@page}{95}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces PCA vs Linear regression\relax }}{94}{figure.caption.118}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{PCA as a generative model}{94}{section*.119}\protected@file@percent }
\pbs@newkey{pbs@last@page}{96}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Autoencoders}{95}{section.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Autoencoder architecture.\relax }}{95}{figure.caption.124}\protected@file@percent }
\newlabel{fig:autoencoder}{{8.4}{95}{Autoencoder architecture.\relax }{figure.caption.124}{}}
\newlabel{fig:autoencoder@cref}{{[figure][4][8]8.4}{[1][95][]95}}
\pbs@newkey{pbs@last@page}{97}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Manifolds}{96}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Manifold hypothesis}{96}{section*.125}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Manifolds}{96}{section*.126}\protected@file@percent }
\pbs@newkey{pbs@last@page}{98}
\@writefile{toc}{\contentsline {paragraph}{Differential geometry}{97}{section*.127}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces An example of differential geometry.\relax }}{97}{figure.caption.128}\protected@file@percent }
\newlabel{fig:bunny}{{8.5}{97}{An example of differential geometry.\relax }{figure.caption.128}{}}
\newlabel{fig:bunny@cref}{{[figure][5][8]8.5}{[1][97][]97}}
\pbs@newkey{pbs@last@page}{99}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces An atlas of e.g. $6$ charts is able to correctly represent Earth. Notice two would suffice, but in general the minimum number is not trivial to compute.\relax }}{98}{figure.caption.129}\protected@file@percent }
\newlabel{fig:sphere}{{8.6}{98}{An atlas of e.g. $6$ charts is able to correctly represent Earth. Notice two would suffice, but in general the minimum number is not trivial to compute.\relax }{figure.caption.129}{}}
\newlabel{fig:sphere@cref}{{[figure][6][8]8.6}{[1][97][]98}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces Chart representation.\relax }}{98}{figure.caption.130}\protected@file@percent }
\newlabel{fig:chart}{{8.7}{98}{Chart representation.\relax }{figure.caption.130}{}}
\newlabel{fig:chart@cref}{{[figure][7][8]8.7}{[1][98][]98}}
\@writefile{toc}{\contentsline {paragraph}{Decoders as a chart}{98}{section*.131}\protected@file@percent }
\pbs@newkey{pbs@last@page}{100}
\@writefile{toc}{\contentsline {paragraph}{Limitations of autoencoders}{99}{section*.132}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Variational Autoencoders (VAE)}{99}{section.8.3}\protected@file@percent }
\pbs@newkey{pbs@last@page}{101}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Variational Autoencoders with a multivariate Gaussian distribution.\relax }}{100}{figure.caption.133}\protected@file@percent }
\newlabel{fig:gauss_variationalAE}{{8.8}{100}{Variational Autoencoders with a multivariate Gaussian distribution.\relax }{figure.caption.133}{}}
\newlabel{fig:gauss_variationalAE@cref}{{[figure][8][8]8.8}{[1][99][]100}}
\newlabel{eq:prob_enc}{{8.28}{100}{Variational Autoencoders (VAE)}{equation.8.3.28}{}}
\newlabel{eq:prob_enc@cref}{{[equation][28][8]8.28}{[1][100][]100}}
\pbs@newkey{pbs@last@page}{102}
\newlabel{eq:opt_prob_enc}{{8.32}{101}{Variational Autoencoders (VAE)}{equation.8.3.32}{}}
\newlabel{eq:opt_prob_enc@cref}{{[equation][32][8]8.32}{[1][101][]101}}
\pbs@newkey{pbs@last@page}{103}
\newlabel{eq:var_enc_loss}{{8.47}{102}{Variational Autoencoders (VAE)}{equation.8.3.47}{}}
\newlabel{eq:var_enc_loss@cref}{{[equation][47][8]8.47}{[1][102][]102}}
\newlabel{eq:11:3:prior-z}{{8.48}{102}{Variational Autoencoders (VAE)}{equation.8.3.48}{}}
\newlabel{eq:11:3:prior-z@cref}{{[equation][48][8]8.48}{[1][102][]102}}
\pbs@newkey{pbs@last@page}{104}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces Variational autoencoder. The code $\vb {z}$ is sampled from $\mathcal  {N}(\bm  {\mu },\bm  {\sigma })$, where $\bm  {\mu },\bm  {\sigma }$ depend on the incoming datapoint $\vb {x}$ but globally follows $\mathcal  {N}(\vb {0}, \vb {I})$.\relax }}{103}{figure.caption.140}\protected@file@percent }
\newlabel{fig:variational_ae}{{8.9}{103}{Variational autoencoder. The code $\vb {z}$ is sampled from $\mathcal {N}(\bm {\mu },\bm {\sigma })$, where $\bm {\mu },\bm {\sigma }$ depend on the incoming datapoint $\vb {x}$ but globally follows $\mathcal {N}(\vb {0}, \vb {I})$.\relax }{figure.caption.140}{}}
\newlabel{fig:variational_ae@cref}{{[figure][9][8]8.9}{[1][102][]103}}
\newlabel{eq:small_gauss}{{8.49}{103}{Variational Autoencoders (VAE)}{equation.8.3.49}{}}
\newlabel{eq:small_gauss@cref}{{[equation][49][8]8.49}{[1][103][]103}}
\@writefile{toc}{\contentsline {paragraph}{VAE: Training}{103}{section*.141}\protected@file@percent }
\pbs@newkey{pbs@last@page}{105}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces The role of the regularizer in VAEs, on the MNIST dataset. On the left we have no regularization, so regular autoencoders, in which the latent space is well behaved only in regions in which codes were assigned to observed data. In the center, only the regularization, so the codes are globally distributed as a normal Gaussian, but there is no correspondance with the input data, so we see codes for a digit mixed with codes for another digit. On the right, the combination of the two gives raise to a compact, smoothly changing distribution that has also localized regions (\emph  {modes}) corresponding to certain digits.\relax }}{104}{figure.caption.142}\protected@file@percent }
\newlabel{fig:regularizer_vae}{{8.10}{104}{The role of the regularizer in VAEs, on the MNIST dataset. On the left we have no regularization, so regular autoencoders, in which the latent space is well behaved only in regions in which codes were assigned to observed data. In the center, only the regularization, so the codes are globally distributed as a normal Gaussian, but there is no correspondance with the input data, so we see codes for a digit mixed with codes for another digit. On the right, the combination of the two gives raise to a compact, smoothly changing distribution that has also localized regions (\emph {modes}) corresponding to certain digits.\relax }{figure.caption.142}{}}
\newlabel{fig:regularizer_vae@cref}{{[figure][10][8]8.10}{[1][103][]104}}
\pbs@newkey{pbs@last@page}{106}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces Reparametrization trick.\relax }}{105}{figure.caption.143}\protected@file@percent }
\newlabel{fig:11:3:rsample}{{8.11}{105}{Reparametrization trick.\relax }{figure.caption.143}{}}
\newlabel{fig:11:3:rsample@cref}{{[figure][11][8]8.11}{[1][104][]105}}
\@writefile{toc}{\contentsline {paragraph}{VAE: Testing}{105}{section*.144}\protected@file@percent }
\pbs@newkey{pbs@last@page}{107}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces Sampling on the boundaries of the distribution.\relax }}{106}{figure.caption.145}\protected@file@percent }
\newlabel{fig:unlikely_samples}{{8.12}{106}{Sampling on the boundaries of the distribution.\relax }{figure.caption.145}{}}
\newlabel{fig:unlikely_samples@cref}{{[figure][12][8]8.12}{[1][105][]106}}
\pbs@newkey{pbs@last@page}{108}
\@writefile{toc}{\contentsline {paragraph}{VAE interpolation}{107}{section*.146}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces Variational Autoencoder interpolation.\relax }}{107}{figure.caption.147}\protected@file@percent }
\newlabel{fig:interpolation}{{8.13}{107}{Variational Autoencoder interpolation.\relax }{figure.caption.147}{}}
\newlabel{fig:interpolation@cref}{{[figure][13][8]8.13}{[1][106][]107}}
\pbs@newkey{pbs@last@page}{109}
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces Interpolation between different images of digits.\relax }}{108}{figure.caption.148}\protected@file@percent }
\newlabel{fig:interp-mnist}{{8.14}{108}{Interpolation between different images of digits.\relax }{figure.caption.148}{}}
\newlabel{fig:interp-mnist@cref}{{[figure][14][8]8.14}{[1][107][]108}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Disentaglement.\relax }}{108}{figure.caption.149}\protected@file@percent }
\newlabel{fig:disentanglement}{{8.15}{108}{Disentaglement.\relax }{figure.caption.149}{}}
\newlabel{fig:disentanglement@cref}{{[figure][15][8]8.15}{[1][108][]108}}
\pbs@newkey{pbs@last@page}{110}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Geometric deep learning}{109}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Introduction}{109}{section.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometric deep learning vs manifold learning}{109}{section*.150}\protected@file@percent }
\pbs@newkey{pbs@last@page}{111}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces On the left, we have only data, i.e. the intensity values for the three channels for every pixel, that can be expressed as a vector-to-vector function. On the right, we have only the structure, but that still encodes relevant information, even without the data. We want to capture both.\relax }}{110}{figure.caption.151}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Static vs dynamic domain.\relax }}{110}{figure.caption.152}\protected@file@percent }
\newlabel{fig:domain}{{9.2}{110}{Static vs dynamic domain.\relax }{figure.caption.152}{}}
\newlabel{fig:domain@cref}{{[figure][2][9]9.2}{[1][110][]110}}
\pbs@newkey{pbs@last@page}{112}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}First examples}{111}{section.9.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Discrete manifold.\relax }}{111}{figure.caption.153}\protected@file@percent }
\newlabel{fig:disc-manifold}{{9.3}{111}{Discrete manifold.\relax }{figure.caption.153}{}}
\newlabel{fig:disc-manifold@cref}{{[figure][3][9]9.3}{[1][111][]111}}
\@writefile{toc}{\contentsline {paragraph}{Multi-view CNN}{111}{section*.154}\protected@file@percent }
\pbs@newkey{pbs@last@page}{113}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Multi-view CNN.\relax }}{112}{figure.caption.155}\protected@file@percent }
\newlabel{fig:multi-viewCNN}{{9.4}{112}{Multi-view CNN.\relax }{figure.caption.155}{}}
\newlabel{fig:multi-viewCNN@cref}{{[figure][4][9]9.4}{[1][111][]112}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Application of multi-view CNN.\relax }}{112}{figure.caption.156}\protected@file@percent }
\newlabel{fig:app-multiviewCNN}{{9.5}{112}{Application of multi-view CNN.\relax }{figure.caption.156}{}}
\newlabel{fig:app-multiviewCNN@cref}{{[figure][5][9]9.5}{[1][112][]112}}
\@writefile{toc}{\contentsline {paragraph}{$3$D ShapeNets}{112}{section*.157}\protected@file@percent }
\pbs@newkey{pbs@last@page}{114}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces $3$D ShapeNet.\relax }}{113}{figure.caption.158}\protected@file@percent }
\newlabel{fig:3d-shapenet}{{9.6}{113}{$3$D ShapeNet.\relax }{figure.caption.158}{}}
\newlabel{fig:3d-shapenet@cref}{{[figure][6][9]9.6}{[1][112][]113}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces $3$D primitives\relax }}{113}{figure.caption.159}\protected@file@percent }
\newlabel{fig:primitives}{{9.7}{113}{$3$D primitives\relax }{figure.caption.159}{}}
\newlabel{fig:primitives@cref}{{[figure][7][9]9.7}{[1][113][]113}}
\pbs@newkey{pbs@last@page}{115}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Challenges}{114}{section.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces Extrinsic vs Intrinsic.\relax }}{114}{figure.caption.160}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces Extrinsic vs Intrisc.\relax }}{114}{figure.caption.161}\protected@file@percent }
\pbs@newkey{pbs@last@page}{116}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces Local ambiguity.\relax }}{115}{figure.caption.162}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces Idea of convolution on a mesh.\relax }}{115}{figure.caption.163}\protected@file@percent }
\pbs@newkey{pbs@last@page}{117}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces Not really clear how to define non-Euclidean convolution on graphs...\relax }}{116}{figure.caption.164}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Generalize Convolution}{116}{section.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Global parametrization}{116}{subsection.9.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces Global parametrization.\relax }}{116}{figure.caption.165}\protected@file@percent }
\newlabel{fig:param}{{9.13}{116}{Global parametrization.\relax }{figure.caption.165}{}}
\newlabel{fig:param@cref}{{[figure][13][9]9.13}{[1][116][]116}}
\pbs@newkey{pbs@last@page}{118}
\@writefile{lof}{\contentsline {figure}{\numberline {9.14}{\ignorespaces Hairy ball theorem. On a sphere we cannot have a translation field without singularities, meaning points in which the translation is not well defined. In this example they are the poles. For other surfaces, like the torus, instead we have no problems.\relax }}{117}{figure.caption.166}\protected@file@percent }
\newlabel{fig:hairy-ball}{{9.14}{117}{Hairy ball theorem. On a sphere we cannot have a translation field without singularities, meaning points in which the translation is not well defined. In this example they are the poles. For other surfaces, like the torus, instead we have no problems.\relax }{figure.caption.166}{}}
\newlabel{fig:hairy-ball@cref}{{[figure][14][9]9.14}{[1][117][]117}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.15}{\ignorespaces Parametrization of a torus.\relax }}{117}{figure.caption.167}\protected@file@percent }
\newlabel{fig:torus}{{9.15}{117}{Parametrization of a torus.\relax }{figure.caption.167}{}}
\newlabel{fig:torus@cref}{{[figure][15][9]9.15}{[1][117][]117}}
\pbs@newkey{pbs@last@page}{119}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Spectral convolution}{118}{subsection.9.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Convolution in the Fourier domain}{118}{section*.168}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The connection with the Laplacian}{118}{section*.169}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.16}{\ignorespaces Neighborhood of a vertex.\relax }}{118}{figure.caption.170}\protected@file@percent }
\newlabel{eq:12:4:2:graph-laplacian}{{9.3}{118}{The connection with the Laplacian}{equation.9.4.3}{}}
\newlabel{eq:12:4:2:graph-laplacian@cref}{{[equation][3][9]9.3}{[1][118][]118}}
\pbs@newkey{pbs@last@page}{120}
\newlabel{eq:12:4:2:non-euclidean-fourier}{{9.6}{119}{The connection with the Laplacian}{equation.9.4.6}{}}
\newlabel{eq:12:4:2:non-euclidean-fourier@cref}{{[equation][6][9]9.6}{[1][119][]119}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.17}{\ignorespaces Decomposition of a function $f$ over a manifold (the mesh of a humanoid) as the terms of a Fourier series. The contributions of every eigenfunction $\phi _i$ of the Laplacian over the Manifold (Fourier basis), weighted by the appropriate eigenvalue $\alpha _1 = \hat  {f}_i$ (Fourier coefficient) make up the original function $f$.\relax }}{119}{figure.caption.171}\protected@file@percent }
\pbs@newkey{pbs@last@page}{121}
\@writefile{toc}{\contentsline {paragraph}{Discrete spectral convolution}{120}{section*.172}\protected@file@percent }
\pbs@newkey{pbs@last@page}{122}
\newlabel{eq:12:4:2:spectral-discrete-conv}{{9.24}{121}{Discrete spectral convolution}{equation.9.4.24}{}}
\newlabel{eq:12:4:2:spectral-discrete-conv@cref}{{[equation][24][9]9.24}{[1][121][]121}}
\@writefile{toc}{\contentsline {paragraph}{Limitations of the spectral convolution}{121}{section*.173}\protected@file@percent }
\pbs@newkey{pbs@last@page}{123}
\@writefile{toc}{\contentsline {subparagraph}{The spectral convolution is not shift invariant}{122}{section*.174}\protected@file@percent }
\newlabel{eq:12:4:2:circulant}{{9.25}{122}{The spectral convolution is not shift invariant}{equation.9.4.25}{}}
\newlabel{eq:12:4:2:circulant@cref}{{[equation][25][9]9.25}{[1][121][]122}}
\@writefile{toc}{\contentsline {subparagraph}{Spectral convolution is domain dependent}{122}{section*.175}\protected@file@percent }
\pbs@newkey{pbs@last@page}{124}
\@writefile{lof}{\contentsline {figure}{\numberline {9.18}{\ignorespaces Signal $\vb {x}$ over the horse's mesh.\relax }}{123}{figure.caption.176}\protected@file@percent }
\newlabel{fig:12:4:2:horse-signal}{{9.18}{123}{Signal $\vb {x}$ over the horse's mesh.\relax }{figure.caption.176}{}}
\newlabel{fig:12:4:2:horse-signal@cref}{{[figure][18][9]9.18}{[1][122][]123}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.19}{\ignorespaces Edge detection on a mesh. \relax }}{123}{figure.caption.177}\protected@file@percent }
\newlabel{fig:12:4:2:spectral-horse}{{9.19}{123}{Edge detection on a mesh. \relax }{figure.caption.177}{}}
\newlabel{fig:12:4:2:spectral-horse@cref}{{[figure][19][9]9.19}{[1][123][]123}}
\pbs@newkey{pbs@last@page}{125}
\zref@newlabel{anim@abspage5}{\abspage{125}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.20}{\ignorespaces As you can imagine, our nice edge detection model fails miserably\relax }}{124}{figure.caption.178}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Spectral convolution is not local}{124}{section*.179}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variants of the spectral convolution}{124}{section*.180}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Spectral CNN with smooth spectral filters}{124}{section*.181}\protected@file@percent }
\pbs@newkey{pbs@last@page}{126}
\@writefile{lof}{\contentsline {figure}{\numberline {9.21}{\ignorespaces Linear combination of $10$ Gaussian kernel functions, that yields a smooth transformation of the Laplacian eigenvalues. This will translate to a localized filter.\relax }}{125}{figure.caption.182}\protected@file@percent }
\newlabel{eq:12:4:2:smooth-spectral-conv-1}{{9.35}{125}{Spectral CNN with smooth spectral filters}{equation.9.4.35}{}}
\newlabel{eq:12:4:2:smooth-spectral-conv-1@cref}{{[equation][35][9]9.35}{[1][125][]125}}
\pbs@newkey{pbs@last@page}{127}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Spatial convolution}{126}{subsection.9.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.22}{\ignorespaces Local system of coordinates.\relax }}{126}{figure.caption.183}\protected@file@percent }
\newlabel{fig:local-coord}{{9.22}{126}{Local system of coordinates.\relax }{figure.caption.183}{}}
\newlabel{fig:local-coord@cref}{{[figure][22][9]9.22}{[1][126][]126}}
\pbs@newkey{pbs@last@page}{128}
\@writefile{lof}{\contentsline {figure}{\numberline {9.23}{\ignorespaces Local system of coordinates.\relax }}{127}{figure.caption.184}\protected@file@percent }
\newlabel{fig:local-coord-weights}{{9.23}{127}{Local system of coordinates.\relax }{figure.caption.184}{}}
\newlabel{fig:local-coord-weights@cref}{{[figure][23][9]9.23}{[1][127][]127}}
\pbs@newkey{pbs@last@page}{129}
\@writefile{lof}{\contentsline {figure}{\numberline {9.24}{\ignorespaces Different possible weighting mechanism to localize functions in the neighborhood of a point.\relax }}{128}{figure.caption.185}\protected@file@percent }
\pbs@newkey{pbs@last@page}{130}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Adversarial training}{129}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Generative Adversarial Networks}{129}{section.10.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{131}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}Introduction}{130}{subsection.10.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Decoder as a generator of fake samples.\relax }}{130}{figure.caption.186}\protected@file@percent }
\newlabel{fig:13:1:gan}{{10.1}{130}{Decoder as a generator of fake samples.\relax }{figure.caption.186}{}}
\newlabel{fig:13:1:gan@cref}{{[figure][1][10]10.1}{[1][129][]130}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces GAN.\relax }}{130}{figure.caption.187}\protected@file@percent }
\pbs@newkey{pbs@last@page}{132}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Formalization}{131}{subsection.10.1.2}\protected@file@percent }
\newlabel{eq:13:1:2:d-obj}{{10.2}{131}{Formalization}{equation.10.1.2}{}}
\newlabel{eq:13:1:2:d-obj@cref}{{[equation][2][10]10.2}{[1][131][]131}}
\newlabel{eq:success_rate}{{10.3}{131}{Formalization}{equation.10.1.3}{}}
\newlabel{eq:success_rate@cref}{{[equation][3][10]10.3}{[1][131][]131}}
\pbs@newkey{pbs@last@page}{133}
\newlabel{eq:generator}{{10.4}{132}{Formalization}{equation.10.1.4}{}}
\newlabel{eq:generator@cref}{{[equation][4][10]10.4}{[1][132][]132}}
\newlabel{eq:13:1:2:d-functional}{{10.5}{132}{Formalization}{equation.10.1.5}{}}
\newlabel{eq:13:1:2:d-functional@cref}{{[equation][5][10]10.5}{[1][132][]132}}
\newlabel{eq:short-max-discr}{{10.7}{132}{Formalization}{equation.10.1.7}{}}
\newlabel{eq:short-max-discr@cref}{{[equation][7][10]10.7}{[1][132][]132}}
\newlabel{eq:13:1:2:d-eq}{{10.11}{132}{Formalization}{equation.10.1.11}{}}
\newlabel{eq:13:1:2:d-eq@cref}{{[equation][11][10]10.11}{[1][132][]132}}
\pbs@newkey{pbs@last@page}{134}
\newlabel{eq:13:1:2:g-objective}{{10.16}{133}{Formalization}{equation.10.1.16}{}}
\newlabel{eq:13:1:2:g-objective@cref}{{[equation][16][10]10.16}{[1][133][]133}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Adversarial attacks}{133}{section.10.2}\protected@file@percent }
\pbs@newkey{pbs@last@page}{135}
\@writefile{toc}{\contentsline {paragraph}{Examples}{134}{section*.188}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces An example of malicious application. \relax }}{134}{figure.caption.189}\protected@file@percent }
\newlabel{fig:malicious}{{10.3}{134}{An example of malicious application. \relax }{figure.caption.189}{}}
\newlabel{fig:malicious@cref}{{[figure][3][10]10.3}{[1][134][]134}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Imperceptible adversarial attack.\relax }}{134}{figure.caption.190}\protected@file@percent }
\newlabel{fig:schoolbus}{{10.4}{134}{Imperceptible adversarial attack.\relax }{figure.caption.190}{}}
\newlabel{fig:schoolbus@cref}{{[figure][4][10]10.4}{[1][134][]134}}
\@writefile{toc}{\contentsline {paragraph}{Perception}{134}{section*.191}\protected@file@percent }
\pbs@newkey{pbs@last@page}{136}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Different perturbations for different tasks.\relax }}{135}{figure.caption.192}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Types of attacks}{135}{section*.193}\protected@file@percent }
\pbs@newkey{pbs@last@page}{137}
\@writefile{toc}{\contentsline {subparagraph}{Targeted attacks}{136}{section*.194}\protected@file@percent }
\pbs@newkey{pbs@last@page}{138}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Intuition of the definition of $f$.\relax }}{137}{figure.caption.195}\protected@file@percent }
\newlabel{fig:intuition}{{10.6}{137}{Intuition of the definition of $f$.\relax }{figure.caption.195}{}}
\newlabel{fig:intuition@cref}{{[figure][6][10]10.6}{[1][137][]137}}
\@writefile{toc}{\contentsline {subparagraph}{Untargeted attacks}{137}{section*.196}\protected@file@percent }
\newlabel{eq:untargeted}{{10.27}{137}{Untargeted attacks}{equation.10.2.27}{}}
\newlabel{eq:untargeted@cref}{{[equation][27][10]10.27}{[1][137][]137}}
\pbs@newkey{pbs@last@page}{139}
\@writefile{toc}{\contentsline {paragraph}{Vulnerability}{138}{section*.197}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces (Nonlinear) Decision boundary for an arbitrary learning model.\relax }}{138}{figure.caption.198}\protected@file@percent }
\newlabel{fig:boundary}{{10.7}{138}{(Nonlinear) Decision boundary for an arbitrary learning model.\relax }{figure.caption.198}{}}
\newlabel{fig:boundary@cref}{{[figure][7][10]10.7}{[1][138][]138}}
\pbs@newkey{pbs@last@page}{140}
\@writefile{toc}{\contentsline {paragraph}{Universal perturbations}{139}{section*.199}\protected@file@percent }
\pbs@newkey{pbs@last@page}{141}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces The perturbation in the center misclassifies all the images except for the dog.\relax }}{140}{figure.caption.200}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Closing remarks}{140}{section*.201}\protected@file@percent }
\pbs@newkey{pbs@last@page}{142}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Linear Algebra}{141}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Vector spaces}{141}{section.A.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{143}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces An example of surface.\relax }}{142}{figure.caption.202}\protected@file@percent }
\pbs@newkey{pbs@last@page}{144}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Basis}{143}{subsection.A.1.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{145}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Linear maps}{144}{section.A.2}\protected@file@percent }
\pbs@newkey{pbs@last@page}{146}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}Matrices}{145}{subsection.A.2.1}\protected@file@percent }
\pbs@newkey{pbs@last@page}{147}
\pbs@newkey{pbs@last@page}{148}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Matrix meta-mechanics}{147}{section.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Transpose and inverse}{147}{thmt@dummyctr.dummy.11}\protected@file@percent }
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {defn}{\numberline {A.1}Definition\thmtformatoptarg {Symmetric matrix}}{147}{defn.A.1}\protected@file@percent }
\@writefile{loe}{\contentsline {defn}{\numberline {A.2}Definition\thmtformatoptarg {Orthogonal Matrix}}{147}{defn.A.2}\protected@file@percent }
\@writefile{loe}{\contentsline {claim}{\numberline {A.1}Claim}{147}{claim.A.1}\protected@file@percent }
\newlabel{cl:orthogonal}{{A.1}{147}{}{claim.A.1}{}}
\newlabel{cl:orthogonal@cref}{{[claim][1][2147483647,1]A.1}{[1][147][]147}}
\@writefile{toc}{\contentsline {paragraph}{Products}{147}{section*.206}\protected@file@percent }
\pbs@newkey{pbs@last@page}{149}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Other properties}{148}{section.A.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Projection of $\mathbf  {a}$ on $\mathbf  {b}$\relax }}{148}{figure.caption.207}\protected@file@percent }
\newlabel{fig:A_proj}{{A.2}{148}{Projection of $\mathbf {a}$ on $\mathbf {b}$\relax }{figure.caption.207}{}}
\newlabel{fig:A_proj@cref}{{[figure][2][2147483647,1]A.2}{[1][148][]148}}
\pbs@newkey{pbs@last@page}{150}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Information Theory}{149}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:appendix-A}{{B}{149}{Information Theory}{appendix.B}{}}
\newlabel{sec:appendix-A@cref}{{[appendix][2][2147483647]B}{[1][149][]149}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Entropy}{149}{section.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intuition}{149}{section*.208}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition}{149}{section*.209}\protected@file@percent }
\pbs@newkey{pbs@last@page}{151}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Kullback-Leibler divergence}{150}{section.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intuition}{150}{section*.210}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition}{150}{section*.211}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Properties}{150}{section*.212}\protected@file@percent }
\newlabel{eq:KL-divergence}{{B.6}{150}{Properties}{equation.B.2.6}{}}
\newlabel{eq:KL-divergence@cref}{{[equation][6][2147483647,2]B.6}{[1][150][]150}}
\pbs@newkey{pbs@last@page}{152}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Fourier Analysis}{151}{appendix.C}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:appendix:fourier}{{C}{151}{Fourier Analysis}{appendix.C}{}}
\newlabel{sec:appendix:fourier@cref}{{[appendix][3][2147483647]C}{[1][151][]151}}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Signals}{151}{section.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C.2}Fourier series}{151}{section.C.2}\protected@file@percent }
\pbs@newkey{pbs@last@page}{153}
\pbs@newkey{pbs@last@page}{154}
\@writefile{toc}{\contentsline {section}{\numberline {C.3}Fourier transform}{153}{section.C.3}\protected@file@percent }
\pbs@newkey{pbs@last@page}{155}
\newlabel{eq:fourier:inverse-transform}{{C.16}{154}{Fourier transform}{equation.C.3.16}{}}
\newlabel{eq:fourier:inverse-transform@cref}{{[equation][16][2147483647,3]C.16}{[1][154][]154}}
\newlabel{eq:fourier:transform}{{C.17}{154}{Fourier transform}{equation.C.3.17}{}}
\newlabel{eq:fourier:transform@cref}{{[equation][17][2147483647,3]C.17}{[1][154][]154}}
\pbs@newkey{pbs@last@page}{156}
\@writefile{toc}{\contentsline {section}{\numberline {C.4}Properties}{155}{section.C.4}\protected@file@percent }
\pbs@newkey{pbs@last@page}{157}
\@writefile{toc}{\contentsline {chapter}{\numberline {D}Spectral Graph Theory}{156}{appendix.D}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Real-valued functions}{156}{section*.213}\protected@file@percent }
